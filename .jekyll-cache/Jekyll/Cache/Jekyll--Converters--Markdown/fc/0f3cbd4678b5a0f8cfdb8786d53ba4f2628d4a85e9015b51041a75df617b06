I"˙ó<p>Q-learning is a model-free reinforcement learning algorithm that is used to find an optimal policy in a Markov decision process problem. The algorithm learns the <strong>action-value function</strong> Q=Q(s, a), which describes the value corresponding to a given action, carried out on a given state. Q-learning can work both <em>on-policy</em> and <em>off-policy</em>, and also somewhere in between. A Q-learning variant working <em>on-policy</em> is the SARSA (<strong>S</strong>tate <strong>A</strong>ction <strong>R</strong>eward <strong>S</strong>tate <strong>A</strong>ction) algorithm, which, as its name suggests, is based on state-action pairs. Moreover, the expected SARSA algorithm can work both  <em>on-policy</em> and <em>off-policy</em>, depending on the choice of the policy. Here we will not describe the algorithms, but we will just use them to solve two different problems (following the <a href="http://incompleteideas.net/book/the-book-2nd.html">Richard S. Sutton and Andrew G. Barto book</a>).</p>

<h2 id="the-cliff">The cliff</h2>
<p>In the following we use a simple toy model, called ‚Äúthe cliff‚Äù by <a href="http://incompleteideas.net/book/the-book-2nd.html">Richard Sutton and Andrew Barto</a>), to compare Q-learning and SARSA. The task is simple: we have a 12x4 grid, the agent starts from the bottom left corner and its task is to reach the bottom right one. The agent can move in every direction and gets a reward of -1 for each step. If the agent reaches one of the bottom tiles, which is neither the starting nor the ending one, it gets a reward of -100 and it is moved to the starting tile (the bottom left one). The task of the reinforcement learning algorithm finds the best policy to maximize the reward. The chosen policy update is the epsilon-greedy policy.</p>

<p>Here are some libraries and configurations we will use</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tqdm</span>
<span class="kn">from</span> <span class="nn">cycler</span> <span class="kn">import</span> <span class="n">cycler</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">'#0c6575'</span><span class="p">,</span> <span class="s">'#bbcbcb'</span><span class="p">,</span> <span class="s">'#23a98c'</span><span class="p">,</span> <span class="s">'#fc7a70'</span><span class="p">,</span><span class="s">'#a07060'</span><span class="p">,</span>
          <span class="s">'#003847'</span><span class="p">,</span> <span class="s">'#FFF7D6'</span><span class="p">,</span> <span class="s">'#5CA4B5'</span><span class="p">,</span> <span class="s">'#eeeeee'</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'axes.prop_cycle'</span><span class="p">]</span> <span class="o">=</span> <span class="n">cycler</span><span class="p">(</span><span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">)</span>

<span class="n">temp_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">'../temp'</span><span class="p">)</span>
</code></pre></div></div>

<p>The possible actions of the agents are:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">do_action</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">action</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">+</span><span class="mi">1</span>
    <span class="k">elif</span> <span class="n">action</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span>
    <span class="k">elif</span> <span class="n">action</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">-</span><span class="mi">1</span>
    <span class="k">elif</span> <span class="n">action</span><span class="o">==</span><span class="mi">3</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span>
        
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">SIZE_X</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">SIZE_Y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">x</span><span class="o">&gt;</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">x</span><span class="o">&lt;</span><span class="p">(</span><span class="n">SIZE_X</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">x</span><span class="o">==</span><span class="p">(</span><span class="n">SIZE_X</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reward</span>
</code></pre></div></div>

<p>The epsilon-greedy action selection is given by:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_greedy_action</span><span class="p">(</span><span class="n">actions</span><span class="p">):</span>
    <span class="n">best_actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">actions</span><span class="o">==</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">actions</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">best_actions</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">get_greedy_action</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="p">:])</span>
</code></pre></div></div>

<p>A single step and optimization is showed below. <code class="highlighter-rouge">Q</code> is the action-value function, <code class="highlighter-rouge">t</code> is the time step, <code class="highlighter-rouge">alg</code> can be set as <em>sarsa</em> or as <em>q</em> to select between q-learning and SARSA algorithms. <code class="highlighter-rouge">alpha</code> is the learning rate and <code class="highlighter-rouge">gamma</code> is the discount factor.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_one_episode</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">return_steps</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">alg</span><span class="o">=</span><span class="s">'sarsa'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">return_steps</span><span class="p">:</span>
        <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">select_action</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">sum_rewards</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">x_next</span><span class="p">,</span> <span class="n">y_next</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">do_action</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_steps</span><span class="p">:</span>
            <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
            <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
        <span class="n">sum_rewards</span> <span class="o">=</span> <span class="n">sum_rewards</span> <span class="o">+</span> <span class="n">reward</span>
        
        <span class="n">action_next</span> <span class="o">=</span> <span class="n">select_action</span><span class="p">(</span><span class="n">x_next</span><span class="p">,</span> <span class="n">y_next</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">alg</span> <span class="o">==</span> <span class="s">'sarsa'</span><span class="p">:</span>
            <span class="n">Q</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">reward</span> <span class="o">+</span>
                    <span class="n">gamma</span><span class="o">*</span><span class="n">Q</span><span class="p">[</span><span class="n">y_next</span><span class="p">,</span> <span class="n">x_next</span><span class="p">,</span> <span class="n">action_next</span><span class="p">]</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">action</span><span class="p">]))</span>
        <span class="k">elif</span> <span class="n">alg</span> <span class="o">==</span> <span class="s">'q'</span><span class="p">:</span>
            <span class="n">Q</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">reward</span> <span class="o">+</span>
                    <span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">y_next</span><span class="p">,</span> <span class="n">x_next</span><span class="p">,</span> <span class="p">:])</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">action</span><span class="p">]))</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x_next</span><span class="p">,</span> <span class="n">y_next</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">action_next</span>
        
        <span class="c1"># target reached
</span>        <span class="k">if</span> <span class="ow">not</span> <span class="n">reward</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">return_steps</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Q</span><span class="p">,</span> <span class="n">sum_rewards</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Q</span><span class="p">,</span> <span class="n">sum_rewards</span>
        
</code></pre></div></div>

<h3 id="the-simulation">The simulation</h3>
<p>Here are some utility functions, used to loop between the sates and to plot the average path taken by the two agents.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">STEPS</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">PARALLEL_SIMULATIONS</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">NUMBER_OF_ACTIONS</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">S_MARKER</span> <span class="o">=</span> <span class="mi">1500</span>
<span class="n">SIZE_X</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">SIZE_Y</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">Q_q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">SIZE_Y</span><span class="p">,</span> <span class="n">SIZE_X</span><span class="p">,</span> <span class="n">NUMBER_OF_ACTIONS</span><span class="p">,</span> <span class="n">PARALLEL_SIMULATIONS</span><span class="p">])</span>
<span class="n">Q_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">SIZE_Y</span><span class="p">,</span> <span class="n">SIZE_X</span><span class="p">,</span> <span class="n">NUMBER_OF_ACTIONS</span><span class="p">,</span> <span class="n">PARALLEL_SIMULATIONS</span><span class="p">])</span>
<span class="n">rewards_q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">STEPS</span><span class="p">,</span> <span class="n">PARALLEL_SIMULATIONS</span><span class="p">])</span>
<span class="n">rewards_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">STEPS</span><span class="p">,</span> <span class="n">PARALLEL_SIMULATIONS</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">loop_states</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Q_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Q_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">yield</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">plot_density_path</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">color</span><span class="p">):</span>
    <span class="n">density_path</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">Q_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Q_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
    
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">):</span>
        <span class="n">density_path</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">density_path</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">density_path</span> <span class="o">=</span> <span class="n">density_path</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">density_path</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loop_states</span><span class="p">():</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'s'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">MARKER_SIZE</span><span class="p">,</span>
                   <span class="n">alpha</span><span class="o">=</span><span class="n">density_path</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        
<span class="k">def</span> <span class="nf">plot_background</span><span class="p">(</span><span class="n">axs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loop_states</span><span class="p">():</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'s'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">S_MARKER</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">8</span><span class="p">])</span>
        <span class="p">[</span><span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'s'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">S_MARKER</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Here‚Äôs the comparison between the SARSA (blue) and the Q-learning algorithm (brown).</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">STEPS</span><span class="p">):</span>
    <span class="n">xs_s</span><span class="p">,</span> <span class="n">ys_s</span><span class="p">,</span> <span class="n">xs_q</span><span class="p">,</span> <span class="n">ys_q</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="p">((</span><span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span><span class="p">),</span> <span class="p">(</span><span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">))</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>

    <span class="c1"># background
</span>    <span class="n">plot_background</span><span class="p">(</span><span class="n">axs</span><span class="o">=</span><span class="p">(</span><span class="n">ax0</span><span class="p">,</span> <span class="n">ax2</span><span class="p">))</span>

    <span class="c1"># sarsa
</span>    <span class="k">for</span> <span class="n">sim_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">PARALLEL_SIMULATIONS</span><span class="p">):</span>
        <span class="n">Q_s</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">sim_index</span><span class="p">],</span> <span class="n">sum_rewards_s</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">run_one_episode</span><span class="p">(</span>
            <span class="n">Q_s</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">sim_index</span><span class="p">],</span> <span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alg</span><span class="o">=</span><span class="s">'sarsa'</span><span class="p">,</span> <span class="n">return_steps</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">xs_s</span><span class="p">,</span> <span class="n">ys_s</span> <span class="o">=</span> <span class="n">xs_s</span> <span class="o">+</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys_s</span> <span class="o">+</span> <span class="n">ys</span>
        <span class="n">rewards_s</span><span class="p">[</span><span class="n">step</span><span class="p">,</span> <span class="n">sim_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum_rewards_s</span>
    <span class="n">plot_density_path</span><span class="p">(</span><span class="n">ax0</span><span class="p">,</span> <span class="n">xs_s</span><span class="p">,</span> <span class="n">ys_s</span><span class="p">,</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax0</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'SARSA -- CLIFF'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'white'</span><span class="p">,</span>
             <span class="n">va</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

    <span class="c1"># q-learning
</span>    <span class="k">for</span> <span class="n">sim_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">PARALLEL_SIMULATIONS</span><span class="p">):</span>
        <span class="n">Q_q</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">sim_index</span><span class="p">],</span> <span class="n">sum_rewards_q</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">run_one_episode</span><span class="p">(</span>
            <span class="n">Q_q</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">sim_index</span><span class="p">],</span> <span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alg</span><span class="o">=</span><span class="s">'q'</span><span class="p">,</span> <span class="n">return_steps</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">xs_q</span><span class="p">,</span> <span class="n">ys_q</span> <span class="o">=</span> <span class="n">xs_q</span> <span class="o">+</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys_q</span> <span class="o">+</span> <span class="n">ys</span>
        <span class="n">rewards_q</span><span class="p">[</span><span class="n">step</span><span class="p">,</span> <span class="n">sim_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum_rewards_q</span>
    <span class="n">plot_density_path</span><span class="p">(</span><span class="n">ax2</span><span class="p">,</span> <span class="n">xs_q</span><span class="p">,</span> <span class="n">ys_q</span><span class="p">,</span> <span class="n">colors</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'Q-learning -- CLIFF'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'white'</span><span class="p">,</span>
             <span class="n">va</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_s</span><span class="p">,</span><span class="mi">1</span><span class="p">)[:</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'SARSA'</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_q</span><span class="p">,</span><span class="mi">1</span><span class="p">)[:</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'Q-learning'</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">150</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">),</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">STEPS</span><span class="p">),</span> <span class="n">xlabel</span><span class="o">=</span><span class="s">'steps'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">'average reward'</span><span class="p">)</span>
    <span class="p">[</span><span class="n">ax1</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="p">(</span><span class="s">'top'</span><span class="p">,</span> <span class="s">'right'</span><span class="p">,</span> <span class="s">'bottom'</span><span class="p">)]</span>
    <span class="p">[</span><span class="n">tl</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s">'none'</span><span class="p">)</span> <span class="k">for</span> <span class="n">tl</span> <span class="ow">in</span> <span class="n">ax1</span><span class="o">.</span><span class="n">get_xticklines</span><span class="p">()];</span>

    <span class="n">ax3</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
    <span class="n">text_description</span> <span class="o">=</span> <span class="n">f</span><span class="s">'''
    Step {step}
    SARSA average reward = {np.mean(rewards_s,1)[step]:.1f}
    Q-learning average reward = {np.mean(rewards_q,1)[step]:.1f}    
    '''</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">text_description</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s">'left'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s">'center'</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">temp_path</span><span class="o">/</span><span class="n">f</span><span class="s">'seq_{step:03d}.jpg'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s">'tight'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div></div>

<p style="text-align:center;"><img src="/asset/images/reinforcement/cliff.gif" alt="cliff" width="800" /></p>

<p>Here we see how the Q-learning algorithm is more greedy but less effective in the long run for this particular problem. It chooses the shorter path, which seems reasonable, but also riskier: given the probability of 10% of a random movement it is possible to fall into the cliff and lose -100 rewards plus the part of the path which was already completed. With that in mind, we can see how the SARSA algorithm is more conservative choosing the longer path, farther from the cliff.</p>
:ET