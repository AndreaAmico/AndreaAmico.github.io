I"‹/<h2 id="why-bayesian-optimization">Why Bayesian optimization</h2>
<p>Bayesian optimization can be used for the global optimization of black-box function with no need to compute derivatives. In particular it can be exploited to efficiently optimize the hyper-parameters defining a neural network model and its training. Since the training of the NN can be very time consuming, it is important to explore the hyper-parameters space wisely. Solution like <a href="https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e">grid search</a> or <a href="https://en.wikipedia.org/wiki/Random_search">random search</a> can be extremely costly and may not be feasible. <a href="https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f">Bayesian optimization</a> is often a much better option, since it takes into account all the previously tested configurations to select the optimal next point to test, choosing the one that maximize the expected information gain.</p>

<h2 id="bayesian-optimization-in-code">Bayesian optimization in code</h2>
<p><strong>NB</strong> The code used to generate all the plots can be found <a href="https://github.com/AndreaAmico/DeepMouse/blob/master/external_library_testing/BasesianOpt.ipynb">here</a>.</p>

<p>Lets give a look at how Bayesian optimization works in practice by using <a href="https://github.com/fmfn/BayesianOptimization">this python library</a> by Fernando Nogueira.</p>

<p>We start by defining our black-box function, which will be completely unknown to the algorithm:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">black</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">sine_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">y</span><span class="o">*</span><span class="mf">0.002</span><span class="p">)</span>
    <span class="n">gaussian_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">50</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2000</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="mi">70</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2000</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">sine_2d</span> <span class="o">+</span> <span class="n">gaussian_2d</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span>

<span class="n">yy</span><span class="p">,</span> <span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">150</span><span class="p">]</span>
<span class="n">zz</span> <span class="o">=</span> <span class="n">black</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">zz</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>
<p style="text-align:center;"><img src="/asset/images/2019-05-08/black_box.png" alt="black box" height="250" width="350" /></p>

<p>Our goal is to find the maximum of this function (most yellow region) by probing the function the least number of times. Lets set up the optimizer:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span>
    <span class="n">f</span><span class="o">=</span><span class="n">black</span><span class="p">,</span>
    <span class="n">pbounds</span><span class="o">=</span><span class="p">{</span><span class="s">'x'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">150</span><span class="p">),</span> <span class="s">'y'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)},</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">utility</span> <span class="o">=</span> <span class="n">UtilityFunction</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">"ei"</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>
<p>The utility function defines the criteria to follow in order to pick the next point to probe. In this case we used <code class="highlighter-rouge">ei</code> which stands for Expected improvement. Lets run the optimization algorithm and see what happens:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">next_point_to_probe</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">suggest</span><span class="p">(</span><span class="n">utility</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">black</span><span class="p">(</span><span class="o">**</span><span class="n">next_point_to_probe</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">next_point_to_probe</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">)</span>
    <span class="c1"># plot the result of the optimizer ...
</span></code></pre></div></div>
<p><img src="/asset/images/2019-05-08/ei.gif" alt="expected improvement" />
The black cross shows the current point being tested by the algorithm, the white dots represent all the previously tested point. The red dot shows the best point tested so far. The goal of the algorithm is to place the red dot on top of the most yellow region as fast as possible, without getting stuck in local maxima.</p>

<p>On the top left panel we plot our black-box function, which represents the ground truth we aim for. On the top right panel we plot the current best prediction of the function given the previously probed points (white dots). As we can see, the more point are acquired, the more faithful to the real black-box function is our prediction.</p>

<p>On the bottom left panel we plot the variance, which quantifies the uncertainty of our model in the parameter space. In the blue regions the variance is low, meaning that we are more confident about the goodness of our model. On the other hand, yellow regions correspond to high variance, i.e. high uncertainty regions. The bottom right panel shows the square difference between the ground truth and our prediction. We notice how the model is much more accurate to describe the black-box function (blue regions) where we tested the function the most.</p>

<hr />

<p>The optimization algorithm which provides the next point to be probed can be chosen to be more <strong>exploitative</strong> or more <strong>explorative</strong>. In the former case the algorithm will be more greedy, trying to spot the maxima as fast as possible. This means that it will more likely explore regions close by the best optima previously find, but it also mean that it is more prone to get stuck into local maxima. On the other hand, the latter approach, is more conservative and probes the parameter space more evenly. It will more likely explore region of high uncertainty (yellow in the bottom left panel) in order to reduce the risk of missing the global maxima of the parameter space. A balance between exploration and exploitation is required and must be carefully considered: high exploitation can converge much faster, but it can converge to the wrong maxima. A good compromise can be starting the research with an exploratory mindset and then switch to exploitative after some iterations.</p>

<hr />

<p>An example of highly explorative algorithm is the following:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># upper confidence bound
</span><span class="n">utility</span> <span class="o">=</span> <span class="n">UtilityFunction</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">"ucb"</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">25.</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/asset/images/2019-05-08/ucb.gif" alt="upper confidence bound" /></p>

<p>As we can see, the parameter space is explored much more evenly, drastically reducing the risk of getting stuck into local maxima, but the convergence is much slower. Notice how the next point to probe is often chosen as the one having the higher estimated variance.</p>

<hr />

<p>On the other hand, an example of highly explorative algorithm is the following:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># probability of improvement
</span><span class="n">utility</span> <span class="o">=</span> <span class="n">UtilityFunction</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">"poi"</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/asset/images/2019-05-08/poi.gif" alt="upper confidence bound" />
The algorithm is now greedy, and try to find the maxima as fast as possible. Notice how it explore regions very close to the last maxima found. We can clearly see the risk of being too greedy: within the initial steps it almost got stuck on the left local maxima.</p>

:ET