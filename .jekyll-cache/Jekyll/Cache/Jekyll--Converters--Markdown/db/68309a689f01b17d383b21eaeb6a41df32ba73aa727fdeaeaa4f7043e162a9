I"ªo<p>The iterative policy evaluation can be used to optimize the decision making in a <strong>Markov Decision Process (MDP)</strong>. In such a problem we have an agent, which can perform actions in an environment. The action brings the agent from a state <em>s</em> to a state <em>s‚Äô</em> and doing so, the agent receives a reward <em>r</em>. The actions available and the reward depend only on the state <em>s</em> the agent is currently in. There is no memory in the system, the state the agent is in contains all the information: the previous state history does not make any difference.</p>

<p>The goal of the agent is to perform actions that maximize the expected reward during a given task. The task can be episodic, i.e. there is an end state like in example a chess game. After the end state is reached the agent state is reset. The task can also be continuous, and it goes on indefinitely. In this latter case, the expected reward must be normalized to avoid divergence, for example by introducing an exponential decay of the rewards while moving into the future.</p>

<h2 id="the-maze">The maze</h2>
<p>Here we use a toy model to show how to optimize the decision-making process of an agent using iterative policy evaluation. Here we will not describe the theory in any detail, which can be found <a href="http://incompleteideas.net/book/the-book-2nd.html">here</a>, but we will just show the code which completes the task.</p>

<p>The problem is simple: we have a 4 by 4 grid, the agent can move in any direction and the goal is to reach to top left corner of the grid. Every movement costs to the agent a negative reward of <code class="highlighter-rouge">-1</code> except for some special cells, identified by darker edges, which have a negative reward of <code class="highlighter-rouge">-5</code>.</p>

<p style="text-align:center;"><img src="/asset/images/reinforcement/iterative_policy/maze.png" alt="maze" width="300" /></p>

<p>To find the optimal path, the agent needs to follow to reach the target from any given state we use the iterative policy evaluation method. This method consists in alternating <strong>value function evaluation</strong> and <strong>policy improvement</strong>. The value function evaluation is obtained by using the <a href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman equations</a>, i.e. estimating the expected reward the agent receives to complete the task with a given policy. A policy is a set of rules which defines the action to be taken in a given state. The initial policy is to move randomly in any given direction.</p>

<p style="text-align:center;"><img src="/asset/images/reinforcement/iterative_policy/iterative_policy_step_0.png" alt="iterative_policy_step_0" width="300" /></p>

<p>Here we see that it costs around <code class="highlighter-rouge">70</code> to reach the goal from a nearby state, while it cost around twice the value starting from the opposite corner. The policy improvement consists of defining a new policy that consists of taking the action which brings the agent in the state which corresponds to the maximum reward.</p>

<p style="text-align:center;"><img src="/asset/images/reinforcement/iterative_policy/iterative_policy_step_1.png" alt="iterative_policy_step_1" width="300" /></p>

<p>As we can see in the figure above, the new policy for the bottom left state is to move the agent left. This because the ‚Äúleft‚Äù state corresponds to a reward of <code class="highlighter-rouge">-137.8</code>, which is greater than the ‚Äúup‚Äù state, which has a reward of <code class="highlighter-rouge">139.4</code>.</p>

<p>By iteratively alternating value function evaluation and policy improvement, we can find the optimal solution to the problem in just 4 iterations as shown in the figures below.</p>

<p style="text-align:center;"><img src="/asset/images/reinforcement/iterative_policy/iterative_policy_step_2.png" alt="iterative_policy_step_2" width="300" /></p>
<p style="text-align:center;"><img src="/asset/images/reinforcement/iterative_policy/iterative_policy_step_3.png" alt="iterative_policy_step_3" width="300" /></p>
<p style="text-align:center;"><img src="/asset/images/reinforcement/iterative_policy/iterative_policy_step_4.png" alt="iterative_policy_step_4" width="300" /></p>
<p style="text-align:center;"><img src="/asset/images/reinforcement/iterative_policy/iterative_policy_step_5.png" alt="iterative_policy_step_5" width="300" /></p>
<p style="text-align:center;"><img src="/asset/images/reinforcement/iterative_policy/iterative_policy_step_6.png" alt="iterative_policy_step_6" width="300" /></p>
<p style="text-align:center;"><img src="/asset/images/reinforcement/iterative_policy/iterative_policy_step_7.png" alt="iterative_policy_step_7" width="300" /></p>

<h2 id="the-code">The code</h2>
<p>Here‚Äôs the code to reproduce the optimization of the path to reach the end of the maze.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">SIZE</span> <span class="o">=</span> <span class="mi">4</span>

<span class="k">def</span> <span class="nf">do_action</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">action</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">y</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">action</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">action</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span>
    <span class="k">if</span> <span class="n">action</span><span class="o">==</span><span class="mi">3</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">y</span>
    
<span class="k">def</span> <span class="nf">action_argmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
    <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">xp</span><span class="p">,</span> <span class="n">yp</span> <span class="o">=</span> <span class="n">do_action</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="p">]</span> <span class="o">+</span> <span class="n">reward</span><span class="p">[</span><span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">values</span><span class="o">==</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">values</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">define_reward</span><span class="p">():</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">SIZE</span><span class="p">,</span><span class="n">SIZE</span><span class="p">])</span>
    <span class="n">reward</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span>
    <span class="n">reward</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span>
    <span class="k">return</span> <span class="n">reward</span>

<span class="k">def</span> <span class="nf">loop_states</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">SIZE</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">SIZE</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span><span class="p">((</span><span class="n">x</span><span class="o">==</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">)):</span>
                <span class="k">yield</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">estimate_value_function</span><span class="p">(</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">delta_max</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">delta</span> <span class="o">&gt;</span> <span class="n">delta_max</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loop_states</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
                <span class="n">xp</span><span class="p">,</span> <span class="n">yp</span> <span class="o">=</span> <span class="n">do_action</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
                <span class="n">v1</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">v1</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">+</span> <span class="n">pi</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">reward</span><span class="p">[</span><span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="p">]</span> <span class="o">+</span> <span class="n">v0</span><span class="p">[</span><span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="p">])</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">v0</span><span class="o">-</span><span class="n">v1</span><span class="p">))</span>
        <span class="n">v0</span><span class="p">,</span> <span class="n">v1</span> <span class="o">=</span> <span class="n">v1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">SIZE</span><span class="p">,</span><span class="n">SIZE</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">v0</span><span class="p">,</span> <span class="n">v1</span>

<span class="k">def</span> <span class="nf">initialize_policy</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">SIZE</span><span class="p">,</span><span class="n">SIZE</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span><span class="o">*</span><span class="mf">0.25</span>

<span class="k">def</span> <span class="nf">initialize_state_values</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">SIZE</span><span class="p">,</span><span class="n">SIZE</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">SIZE</span><span class="p">,</span><span class="n">SIZE</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">plot_value_function</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">pi</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">SIZE</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">SIZE</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="mf">0.2</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">SIZE</span><span class="o">-</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="n">SIZE</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loop_states</span><span class="p">():</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="s">'{v[x, y]:.1f}'</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s">'center'</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'s'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">edgecolors</span> <span class="o">=</span> <span class="s">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">'none'</span><span class="p">,</span>
                   <span class="n">alpha</span><span class="o">=-</span><span class="p">(</span><span class="n">reward</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">reward</span><span class="p">)),</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'s'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">edgecolors</span> <span class="o">=</span> <span class="s">'none'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#0c6575'</span><span class="p">,</span>
                   <span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">v</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">v</span><span class="p">))</span><span class="o">*</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">+</span><span class="mf">0.45</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">+</span><span class="mf">0.25</span><span class="p">),</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s">"-&gt;"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">pi</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">-</span><span class="mf">0.45</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">-</span><span class="mf">0.25</span><span class="p">),</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s">"-&gt;"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">pi</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="mf">0.45</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s">"-&gt;"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">pi</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">""</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">0.45</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s">"-&gt;"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">pi</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'GOAL'</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'green'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>


<span class="k">def</span> <span class="nf">optimize_path</span><span class="p">():</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">define_reward</span><span class="p">()</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="n">initialize_policy</span><span class="p">()</span>
    <span class="n">v0</span><span class="p">,</span> <span class="n">v1</span> <span class="o">=</span> <span class="n">initialize_state_values</span><span class="p">()</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">v0</span><span class="p">,</span> <span class="n">v1</span> <span class="o">=</span> <span class="n">estimate_value_function</span><span class="p">(</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>

        <span class="k">yield</span> <span class="n">plot_value_function</span><span class="p">(</span><span class="n">v0</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loop_states</span><span class="p">():</span>
            <span class="n">pi</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">max_actions</span> <span class="o">=</span> <span class="n">action_argmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">v0</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">max_actions</span><span class="p">:</span>
                <span class="n">pi</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">max_actions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">yield</span> <span class="n">plot_value_function</span><span class="p">(</span><span class="n">v0</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">optimize_path</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
    <span class="nb">next</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div>
:ET