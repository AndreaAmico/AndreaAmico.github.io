I"!<h2 id="setup-python-environment-ubuntu-2004">Setup python environment (Ubuntu 20.04)</h2>

<p>Get <code class="highlighter-rouge">pip</code> and <code class="highlighter-rouge">virtualenv</code> installed:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>python3-pip
<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get <span class="nb">install </span>python3-venv
</code></pre></div></div>
<p>Create a new environment and activate it:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> venv tf
<span class="nb">source </span>tf/bin/activate
</code></pre></div></div>

<p>Install the <code class="highlighter-rouge">tensorflow</code> library and, if needed, <code class="highlighter-rouge">opencv</code>:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>tensorflow
pip <span class="nb">install </span>opencv-contrib-python
</code></pre></div></div>

<h2 id="tensorflow-lite">Tensorflow lite</h2>
<p>If there is no need to train models, a tansorflow lite environment can be used by itself. One needs to select the correct version from <a href="https://www.tensorflow.org/lite/guide/python">this list</a> and install it. For example on Ubuntu 20.04 64bit:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 <span class="nb">install </span>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_x86_64.whl
</code></pre></div></div>

<p>The tensorflow interpreter can now be used as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tflite_runtime.interpreter</span> <span class="k">as</span> <span class="n">tflite</span>
<span class="n">interpreter</span> <span class="o">=</span> <span class="n">tflite</span><span class="o">.</span><span class="n">Interpreter</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">model_file</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="coral-usb-accelerator">Coral usb accelerator</h2>
<p>First install the coral dependencies:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>curl

<span class="nb">echo</span> <span class="s2">"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main"</span> | <span class="nb">sudo tee</span> /etc/apt/sources.list.d/coral-edgetpu.list

curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | <span class="nb">sudo </span>apt-key add -

<span class="nb">sudo </span>apt-get update
</code></pre></div></div>

<p>Finally, install the coral library. There are two versions available, the <strong>standard</strong> and the <strong>max</strong> version, which is faster but the USB device gets hot fast. <strong>NB: only one version can be installed in the system. Installing a different one will override the previous.</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># standard:</span>
<span class="nb">sudo </span>apt-get <span class="nb">install </span>libedgetpu1-std

<span class="c"># max:</span>
<span class="nb">sudo </span>apt-get <span class="nb">install </span>libedgetpu1-max
</code></pre></div></div>

<h2 id="compile-model-for-edge-hardware">Compile model for edge hardware</h2>
<p>To achieve the best performance from the coral USB accelerator we need to compile our tensorflow model. To do so we need to install the <a href="https://coral.ai/docs/edgetpu/compiler/#system-requirements">edgetpu-compiler following the instruction of this link</a>. Notice that we can use the compiler directly from an ARM device such as the raspberry pi. Once the compiler is installed we can convert our model as follows:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>edgetpu_compiler my_model.tflite
</code></pre></div></div>
<p>The output will be <code class="highlighter-rouge">my_model_edge.tflite</code>.</p>

<p>Notice that the compiler requires a quantized tflite model as input. The quantize model can be created during training or by transforming a trained model. All the instructions can be found <a href="https://www.tensorflow.org/lite/performance/post_training_quantization">here</a>. The post-training quantization procedure is the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="k">def</span> <span class="nf">representative_dataset</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">rep_images</span><span class="p">))</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">[</span><span class="n">data</span><span class="p">]</span>
        
<span class="n">saved_model_dir</span> <span class="o">=</span> <span class="s">'my_tensorflow_full_model.model'</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">TFLiteConverter</span><span class="o">.</span><span class="n">from_saved_model</span><span class="p">(</span><span class="n">saved_model_dir</span><span class="p">)</span>
<span class="n">converter</span><span class="o">.</span><span class="n">optimizations</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">Optimize</span><span class="o">.</span><span class="n">DEFAULT</span><span class="p">]</span>
<span class="n">converter</span><span class="o">.</span><span class="n">representative_dataset</span> <span class="o">=</span> <span class="n">representative_dataset</span>
<span class="n">tflite_quant_model</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">()</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="s">'quantized_model.tflite'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">tflite_quant_model</span><span class="p">)</span>
</code></pre></div></div>

<p>where <code class="highlighter-rouge">my_tensorflow_full_model.model</code> is the path to the original tensorflow model and <code class="highlighter-rouge">rep_images</code> is a list containing representative samples the model can expect as input (for example out test data).</p>

:ET