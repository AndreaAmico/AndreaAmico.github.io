I"!<h3 id="summary">Summary</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Fit + transform available
</span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">FactorAnalysis</span>
<span class="kn">from</span> <span class="nn">sklearn.random_projection</span> <span class="kn">import</span> <span class="n">GaussianRandomProjection</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">Isomap</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">LocallyLinearEmbedding</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">FastICA</span>

<span class="c1">## Only fit_transform
</span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">MDS</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">SpectralEmbedding</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="c1">## Both X and y must be provided
</span><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>

<span class="c1">## n_clusters instead of n_components
</span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">FeatureAgglomeration</span>
</code></pre></div></div>

<p>in the following we plot some examples of dimensionality reduction to plot three different datasets in two dimensions. Moreover, we score the goodness of each algorithm for the given dataset using a simple SVM classifier with cross validation:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="k">def</span> <span class="nf">get_score</span><span class="p">(</span><span class="n">X_2D</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_2D</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="s">'{scores.mean():.2f} (+/- {scores.std()*2:.2f})'</span>
</code></pre></div></div>
<h3 id="iris-dataset-sklearn">Iris dataset (sklearn)</h3>
<p style="text-align:center;"><img src="/asset/images/dimensionality_reduction/reduction_iris_1.svg" alt="reduction_iris_1" width="800" /></p>
<p style="text-align:center;"><img src="/asset/images/dimensionality_reduction/reduction_iris_2.svg" alt="reduction_iris_2" width="800" /></p>
<p style="text-align:center;"><img src="/asset/images/dimensionality_reduction/reduction_iris_3.svg" alt="reduction_iris_3" width="800" /></p>

<h3 id="wine-dataset-sklearn">Wine dataset (sklearn)</h3>
<p style="text-align:center;"><img src="/asset/images/dimensionality_reduction/reduction_wine_1.svg" alt="reduction_wine_1" width="800" /></p>
<p style="text-align:center;"><img src="/asset/images/dimensionality_reduction/reduction_wine_2.svg" alt="reduction_wine_2" width="800" /></p>
<p style="text-align:center;"><img src="/asset/images/dimensionality_reduction/reduction_wine_3.svg" alt="reduction_wine_3" width="800" /></p>

<h3 id="digits-dataset-sklearn">Digits dataset (sklearn)</h3>
<p style="text-align:center;"><img src="/asset/images/dimensionality_reduction/reduction_digits_1.svg" alt="reduction_digits_1" width="800" /></p>
<p style="text-align:center;"><img src="/asset/images/dimensionality_reduction/reduction_digits_2.svg" alt="reduction_digits_2" width="800" /></p>
<p style="text-align:center;"><img src="/asset/images/dimensionality_reduction/reduction_digits_3.svg" alt="reduction_digits_3" width="800" /></p>

<h2 id="principal-component-analysis-pca-for-dummies">Principal Component Analysis (PCA) for dummies</h2>
<p>Project all the points along one direction and measure the variance of the projections position. The direction of the first component is chosen by maximizing this variance. The same idea is followed to choose the remaining components, with the constraint to be orthogonal to the previous ones. Mathematically it can be done by solving the eigenvalues problem using the <em>Singular Value Decomposition</em> and selecting as the most important directions the eigenvectors corresponding to the largest eigenvalues:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># X = (U @ np.diag(S) @ V)
# S are the eigenvalues in descending order
</span><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_components</span><span class="p">]</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">U</span> <span class="o">*</span> <span class="n">S</span><span class="p">[:</span><span class="n">n_components</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_2D</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<p style="text-align:center;"><img src="/asset/images/dimensionality_reduction/pca.png" alt="pca_comparison" width="800" /></p>
:ET