<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><style>@font-face {
  font-family: octicons-anchor;
  src: url(https://cdnjs.cloudflare.com/ajax/libs/octicons/4.4.0/font/octicons.woff) format('woff');
}

* {
    box-sizing: border-box;
}

body {
    width: 980px;
    margin-right: auto;
    margin-left: auto;
    color:#333;
    background:#fff;
}

body .markdown-body {
    padding: 45px;
    border: 1px solid #ddd;
    border-radius: 3px;
    word-wrap: break-word;
}

pre {
    font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body {
  -webkit-text-size-adjust: 100%;
  text-size-adjust: 100%;
  color: #333;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background-color: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body strong {
  font-weight: bold;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body input {
  font: 13px / 1.4 Helvetica, arial, nimbussansl, liberationsans, freesans, clean, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
}

.markdown-body a {
  color: #4078c0;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before {
  display: table;
  content: "";
}

.markdown-body hr:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
  font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body .select::-ms-expand {
  opacity: 0;
}

.markdown-body .octicon {
  font: normal normal normal 16px/1 octicons-anchor;
  display: inline-block;
  text-decoration: none;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.markdown-body .octicon-link:before {
  content: '\f05c';
}

.markdown-body:before {
  display: table;
  content: "";
}

.markdown-body:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body a:not([href]) {
  color: inherit;
  text-decoration: none;
}

.markdown-body .anchor {
  display: inline-block;
  padding-right: 2px;
  margin-left: -18px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  color: #000;
  vertical-align: middle;
  visibility: hidden;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  visibility: visible;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h1 .anchor {
  line-height: 1;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 .anchor {
  line-height: 1;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h3 .anchor {
  line-height: 1.2;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h4 .anchor {
  line-height: 1.2;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h5 .anchor {
  line-height: 1.1;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body h6 .anchor {
  line-height: 1.1;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  box-sizing: content-box;
  background-color: #fff;
}

.markdown-body code {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}

.markdown-body .pl-c {
  color: #969896;
}

.markdown-body .pl-c1,
.markdown-body .pl-s .pl-v {
  color: #0086b3;
}

.markdown-body .pl-e,
.markdown-body .pl-en {
  color: #795da3;
}

.markdown-body .pl-s .pl-s1,
.markdown-body .pl-smi {
  color: #333;
}

.markdown-body .pl-ent {
  color: #63a35c;
}

.markdown-body .pl-k {
  color: #a71d5d;
}

.markdown-body .pl-pds,
.markdown-body .pl-s,
.markdown-body .pl-s .pl-pse .pl-s1,
.markdown-body .pl-sr,
.markdown-body .pl-sr .pl-cce,
.markdown-body .pl-sr .pl-sra,
.markdown-body .pl-sr .pl-sre {
  color: #183691;
}

.markdown-body .pl-v {
  color: #ed6a43;
}

.markdown-body .pl-id {
  color: #b52a1d;
}

.markdown-body .pl-ii {
  background-color: #b52a1d;
  color: #f8f8f8;
}

.markdown-body .pl-sr .pl-cce {
  color: #63a35c;
  font-weight: bold;
}

.markdown-body .pl-ml {
  color: #693a17;
}

.markdown-body .pl-mh,
.markdown-body .pl-mh .pl-en,
.markdown-body .pl-ms {
  color: #1d3e81;
  font-weight: bold;
}

.markdown-body .pl-mq {
  color: #008080;
}

.markdown-body .pl-mi {
  color: #333;
  font-style: italic;
}

.markdown-body .pl-mb {
  color: #333;
  font-weight: bold;
}

.markdown-body .pl-md {
  background-color: #ffecec;
  color: #bd2c00;
}

.markdown-body .pl-mi1 {
  background-color: #eaffea;
  color: #55a532;
}

.markdown-body .pl-mdr {
  color: #795da3;
  font-weight: bold;
}

.markdown-body .pl-mo {
  color: #1d3e81;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}

.markdown-body .plan-price-unit {
  color: #767676;
  font-weight: normal;
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 0.35em 0.25em -1.6em;
  vertical-align: middle;
}

.markdown-body .plan-choice {
  padding: 15px;
  padding-left: 40px;
  display: block;
  border: 1px solid #e0e0e0;
  position: relative;
  font-weight: normal;
  background-color: #fafafa;
}

.markdown-body .plan-choice.open {
  background-color: #fff;
}

.markdown-body .plan-choice.open .plan-choice-seat-breakdown {
  display: block;
}

.markdown-body .plan-choice-free {
  border-radius: 3px 3px 0 0;
}

.markdown-body .plan-choice-paid {
  border-radius: 0 0 3px 3px;
  border-top: 0;
  margin-bottom: 20px;
}

.markdown-body .plan-choice-radio {
  position: absolute;
  left: 15px;
  top: 18px;
}

.markdown-body .plan-choice-exp {
  color: #999;
  font-size: 12px;
  margin-top: 5px;
}

.markdown-body .plan-choice-seat-breakdown {
  margin-top: 10px;
  display: none;
}

.markdown-body :checked+.radio-label {
  z-index: 1;
  position: relative;
  border-color: #4078c0;
}

@media print {
  body .markdown-body {
    padding: 0;
    border: none;
  }
}
</style><title>2019-06-15-deep_mouse</title></head><body><article class="markdown-body"><hr>
<h2>
<a id="user-content-layout-posttitle--deep-mousedate---2019-06-15-220000-0100categories-machine_learning" class="anchor" href="#layout-posttitle--deep-mousedate---2019-06-15-220000-0100categories-machine_learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>layout: post
title:  "Deep Mouse"
date:   2019-06-15 22:00:00 +0100
categories: machine_learning</h2>
<p>In this project, we follow the thought processes behind the development of a simple neural network. It is an A-to-Z description of a neural network construction, starting from the acquisition of the dataset up to the evaluation of the model, including errors and steps-back.</p>
<p>The goal of this toy algorithm is to identify if the laptop track-pad is used either with the right or the left hand just by looking at the position of the mouse cursor in time. The scheme of the development process can be sketched as follow:</p>
<p><a href="/asset/images/deep_mouse/deep_learning_development_process.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/deep_learning_development_process.png" alt="Deep learning development process" width="750" style="max-width:100%;"></a></p>
<p>It is an iterative process that periodically ends with the evaluation of the model using the <strong>dev</strong> (development) dataset. The iteration continues until the accuracy obtained on the dev set reaches our goal. Only at this point, the model is tested on the <strong>test</strong> dataset.</p>
<h2>
<a id="user-content-iterations-index" class="anchor" href="#iterations-index" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Iterations index</h2>
<ul>
<li><a href="#identification-of-required-data---01">First model implementation 01</a></li>
<li><a href="#data-pre-processing---02">Data pre-processing 02</a></li>
<li><a href="#algorithm-selection---03">Algorithm optimizer 03</a></li>
<li><a href="#algorithm-selection---04">Neural Network structure 04</a></li>
<li><a href="#algorithm-selection---05">Testing SVM 05</a></li>
<li><a href="#lstm-size-and-learning-rate-with-Bayesopt---06">LSTM size and learning rate with Bayesopt 06</a></li>
<li><a href="#introducing-dropout---07">Introducing dropout 07</a></li>
<li><a href="#checking-training-set-size---08">Checking training set size 08</a></li>
<li><a href="#New-bigger-dataset---09">New, bigger, dataset - 09</a></li>
<li><a href="#gru-instead-of-lstm---10">GRU instead of LSTM - 10</a></li>
<li><a href="#gru-and-overfitting---11">GRU and overfitting - 11</a></li>
<li><a href="#gru-with-mouse-movement---12">GRU with mouse movement - 12</a></li>
<li><a href="#training-size-and-regularizers---13">Training size and regularizers - 13</a></li>
</ul>
<h3>
<a id="user-content-identification-of-required-data---01" class="anchor" href="#identification-of-required-data---01" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Identification of required data - 01<a href="#iterations-index">△</a>
</h3>
<p>To train a model to recognize which hand is moving the mouse, we opted for a supervised learning approach and we, therefore, need labeled data for the training. The structure of the dataset is a <em>.csv</em> file with two columns indicating the <em>x</em> and <em>y</em> absolute coordinate of the mouse cursor on the screen. The position is recorded every ~10ms. To create the training set, we recorded the mouse position for about 10 minutes, first by using the right hand (while reading a technical blog post), then doing the same task by using the left hand. The total amount of samples is 120k (60k right, 60k left).
The data are acquired using the win32gui library and stored in a <em>.txt</em> file via a python script:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> win32gui, time
<span class="pl-k">with</span> <span class="pl-c1">open</span>(<span class="pl-s"><span class="pl-pds">'</span>./data/my_data.txt<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>a<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> f:
    <span class="pl-k">for</span> _ <span class="pl-k">in</span> <span class="pl-c1">range</span>(number_of_samples):
        x, y <span class="pl-k">=</span> win32gui.GetCursorPos()
        f.write(<span class="pl-s"><span class="pl-pds">'</span><span class="pl-c1">{}</span>,<span class="pl-c1">{}</span><span class="pl-cce">\n</span><span class="pl-pds">'</span></span>.format(x, y))
        time.sleep(<span class="pl-c1">0.01</span>)</pre></div>
<p>Right hand data are saved on <em>./data/right.txt</em> file, left hand data on <em>./data/left.txt</em> file.
As shown in the plot below the real-time delay between subsequent acquisitions is not completely constant, moreover, spikes are present.</p>
<p><a href="/asset/images/deep_mouse/data_acquisition_stability.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/data_acquisition_stability.png" alt="Data acquisition stability" width="400" style="max-width:100%;"></a></p>
A possible improvement might be achieved by a pure c acquisition program, which includes a time delay to check every loop. For the moment, since we do not know the impact of a more stable acquisition for the model accuracy, we postpone the problem for later iterations.
<h3>
<a id="user-content-data-pre-processing---01" class="anchor" href="#data-pre-processing---01" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data pre-processing - 01<a href="#iterations-index">△</a>
</h3>
<p>The full 20 minutes dataset is split into batches of 200 points each, corresponding to 2 seconds of mouse position acquisitions. Right, and left-hand data are merged in a single dataset. For the moment we use raw data from the input device, being the absolute coordinate along the horizontal and vertical direction of the screen.</p>
<hr>
<p>Loading the mouse data from the <em>.txt</em> file using <strong>pandas</strong>:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> pandas <span class="pl-k">as</span> pandas
right <span class="pl-k">=</span> pd.read_csv(<span class="pl-s"><span class="pl-pds">"</span>../data/right.txt<span class="pl-pds">"</span></span>, <span class="pl-v">header</span><span class="pl-k">=</span><span class="pl-c1">None</span>).values.tolist()
left <span class="pl-k">=</span> pd.read_csv(<span class="pl-s"><span class="pl-pds">"</span>../data/left.txt<span class="pl-pds">"</span></span>, <span class="pl-v">header</span><span class="pl-k">=</span><span class="pl-c1">None</span>).values.tolist()</pre></div>
<p>Splitting the data in 600 batches containing 200 data-points each:</p>
<div class="highlight highlight-source-python"><pre>batch_size <span class="pl-k">=</span> <span class="pl-c1">200</span>
batch_right <span class="pl-k">=</span> [right[i:i <span class="pl-k">+</span> batch_size] <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">range</span>(<span class="pl-c1">0</span>, <span class="pl-c1">len</span>(right), batch_size)]
batch_left <span class="pl-k">=</span> [left[i:i <span class="pl-k">+</span> batch_size] <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">range</span>(<span class="pl-c1">0</span>, <span class="pl-c1">len</span>(left), batch_size)]</pre></div>
<p>Merging left and right datasets and convert them into <strong>numpy</strong> arrays. Create the target array <code>y</code> using as convention <em>0</em> for batches corresponding tho right hand and <em>1</em> for left hand. The axis of the <code>X</code> array correspond to: (batch index, mouse position in time, mouse coordinate index).</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> numpy <span class="pl-k">as</span> np
X <span class="pl-k">=</span> np.array(batch_right <span class="pl-k">+</span> batch_left)
y <span class="pl-k">=</span> np.array([<span class="pl-c1">0</span>]<span class="pl-k">*</span><span class="pl-c1">len</span>(batch_right) <span class="pl-k">+</span> [<span class="pl-c1">1</span>]<span class="pl-k">*</span><span class="pl-c1">len</span>(batch_left))

<span class="pl-c1">print</span>(<span class="pl-s">f</span><span class="pl-pds">'</span><span class="pl-s">X shape: </span><span class="pl-c1">{</span>X.shape<span class="pl-c1">}</span><span class="pl-cce">\n</span><span class="pl-s">y shape: </span><span class="pl-c1">{</span>y.shape<span class="pl-c1">}</span><span class="pl-pds">'</span>)</pre></div>
<pre lang="text"><code>X shape: (600, 200, 2)
y shape: (600,)
</code></pre>
<p>For the moment the decision of creating batches of 200 data-points is arbitrary and we still do not know if we might need for longer batches to achieve good accuracy. A batch of 200 points, using a 100Hz acquisition rate, means that we need 2 seconds of acquisition before the model can predict a result. In the plot below we show the data contained in a single data batch.</p>
<p><a href="/asset/images/deep_mouse/batch_example.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/batch_example.png" alt="Batch data example" width="500" style="max-width:100%;"></a></p>
<h3>
<a id="user-content-definition-of-the-training-set---01" class="anchor" href="#definition-of-the-training-set---01" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Definition of the training set - 01<a href="#iterations-index">△</a>
</h3>
<p><strong>Training</strong>, <strong>dev</strong> and <strong>test</strong> sets are split in a 70%-15%-15% proportion. The training set is used for training the network, the dev set as a benchmark to optimize the ML algorithm and finally the test set to measure the accuracy of the model. It is important to keep dev and test set separated to avoid the over-fitting of the hyper-parameters of the model on the test set.</p>
<hr>
<p>The splitting between train/dev/test is achieved using the <strong>sklearn</strong> library.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> sklearn.model_selection <span class="pl-k">import</span> train_test_split

X_train, X_test, y_train, y_test <span class="pl-k">=</span> train_test_split(X, y, <span class="pl-v">test_size</span><span class="pl-k">=</span><span class="pl-c1">0.30</span>, <span class="pl-v">random_state</span><span class="pl-k">=</span><span class="pl-c1">1</span>)
X_dev, X_test, y_dev, y_test <span class="pl-k">=</span> train_test_split(X_est, y_est, <span class="pl-v">test_size</span><span class="pl-k">=</span><span class="pl-c1">0.5</span>, <span class="pl-v">random_state</span><span class="pl-k">=</span><span class="pl-c1">1</span>)</pre></div>
<h3>
<a id="user-content-algorithm-selection---01" class="anchor" href="#algorithm-selection---01" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Algorithm selection - 01<a href="#iterations-index">△</a>
</h3>
<p>As a starting point algorithm we opted for a <strong>RNN</strong> (recurrent neural network). In particular, inspired from this <a href="https://www.analyticsvidhya.com/blog/2019/01/introduction-time-series-classification/#" rel="nofollow">blog post</a>, we used a <strong>LSTM</strong> (Long short-term memory) architecture.</p>
<hr>
<p>We implemented the neural network in <strong>keras</strong> using <strong>TensorFlow backend</strong>:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> tensorflow <span class="pl-k">as</span> tf
<span class="pl-k">from</span> keras.models <span class="pl-k">import</span> Sequential
<span class="pl-k">from</span> keras.layers <span class="pl-k">import</span> Dense
<span class="pl-k">from</span> keras.layers <span class="pl-k">import</span> <span class="pl-c1">LSTM</span>

model <span class="pl-k">=</span> Sequential()
model.add(LSTM(<span class="pl-c1">256</span>, <span class="pl-v">input_shape</span><span class="pl-k">=</span>(batch_size, <span class="pl-c1">2</span>)))
model.add(Dense(<span class="pl-c1">1</span>, <span class="pl-v">activation</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>sigmoid<span class="pl-pds">'</span></span>))

model.summary()</pre></div>
<pre lang="text"><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 256)               265216    
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 257       
=================================================================
Total params: 265,473
Trainable params: 265,473
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>The training is performed using <strong>stochastic gradient descent</strong>, in particular using the <strong>Adam</strong> algorithm (short for Adaptive Moment Estimation). We used the <em>accuracy</em> metric and we trained the data for 200 epochs. We save the best model as <em>best_model.pkl</em>. It takes about 8 minutes to train the model using a regular laptop.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> keras.optimizers <span class="pl-k">import</span> Adam
<span class="pl-k">from</span> keras.callbacks <span class="pl-k">import</span> ModelCheckpoint

adam <span class="pl-k">=</span> Adam(<span class="pl-v">lr</span><span class="pl-k">=</span><span class="pl-c1">0.001</span>)
chk <span class="pl-k">=</span> ModelCheckpoint(<span class="pl-s"><span class="pl-pds">'</span>best_model.pkl<span class="pl-pds">'</span></span>, <span class="pl-v">monitor</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>acc<span class="pl-pds">'</span></span>, <span class="pl-v">save_best_only</span><span class="pl-k">=</span><span class="pl-c1">True</span>, <span class="pl-v">mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>max<span class="pl-pds">'</span></span>, <span class="pl-v">verbose</span><span class="pl-k">=</span><span class="pl-c1">0</span>)
model.compile(<span class="pl-v">loss</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>binary_crossentropy<span class="pl-pds">'</span></span>, <span class="pl-v">optimizer</span><span class="pl-k">=</span>adam, <span class="pl-v">metrics</span><span class="pl-k">=</span>[<span class="pl-s"><span class="pl-pds">'</span>accuracy<span class="pl-pds">'</span></span>])
history <span class="pl-k">=</span> model.fit(X_train, y_train, <span class="pl-v">epochs</span><span class="pl-k">=</span><span class="pl-c1">200</span>, <span class="pl-v">batch_size</span><span class="pl-k">=</span><span class="pl-c1">64</span>, <span class="pl-v">callbacks</span><span class="pl-k">=</span>[chk], <span class="pl-v">validation_data</span><span class="pl-k">=</span>(X_dev, y_dev))</pre></div>
<p>The learning process during the gradient descent can be inspected by monitoring the accuracy of the model and the loss function, computed both on the training and the dev set.</p>
<p><a href="/asset/images/deep_mouse/model_01.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/model_01.png" alt="Model 01" width="700" style="max-width:100%;"></a></p>
The **accuracy** describes the ratio between the correct and the total guesses:
```text 
accuracy = number_of_correct_prediction / total_number_of_prediction_made
```
 The loss-function correspond to the **binary cross-entropy**, which is given by:
```text
loss = -(y log(p) + (1-y) log(1-p))
```
where `y` is the target correct binary label (0 for the right hand, 1 for left hand) and `p` is the predicted probability for a given data batch to be a left-hand batch. When the cross-entropy is *1* the model is useless and it is equivalent to a random guess. When it is *0* the model perfectly predicts the target given a single data batch.
<h3>
<a id="user-content-evaluation-of-the-model---01" class="anchor" href="#evaluation-of-the-model---01" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Evaluation of the model - 01<a href="#iterations-index">△</a>
</h3>
<p>A simple validation of the model can be achieved using the <strong>confusion matrix</strong>, which reports the measure of the correct and non-correct labels computed by the model on the dev set.</p>
<p><a href="/asset/images/deep_mouse/conf_matrix_01.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/conf_matrix_01.png" alt="Confusion matrix 01" width="300" style="max-width:100%;"></a></p>
The model produces a very poor result but pieces of information seem to be present in the data. We can also test the model "live", loading it and using the script:
```python
from keras.models import load_model
import win32gui, time
model = load_model(f'../models/model_name.pkl')
<p>def get_batch(batch_size=200):
X_pred = np.ones([1, batch_size, 2])
for index in range(batch_size):
x, y = win32gui.GetCursorPos()
X_pred[0,index,:] = np.array([x, y])
time.sleep(0.01)
return X_pred</p>
<p>time_in_sec = 30
left_guesses = 0
for index in range(1, max(2, int(time_in_sec/2))):
X_pred = get_batch()
pred = model.predict_classes(X_pred)[0][0]
left_guesses += pred
print(f'\rRun_{index}: Current prediction = {"Left " if pred else "Right"}   '+
f'Left probability = {left_guesses/index * 100:.1f}%   '+
f'Right probability = {(1 - left_guesses/index) * 100:.1f}%', end='')</p>
<pre><code>
### Data pre-processing - 02[△](#iterations-index)
An almost useful and safe pre-processing technique on data is their **normalization**. For the moment we used absolute screen coordinate but a very easy improvement is to normalize them by using the screen height and width. Another option is to convert the absolute position of the mouse to the movement performed in 10ms. This might be useful because it simplifies the problem introducing a translational invariance along the coordinate, which looks to be a good symmetry to exploit. We start from this second option:
```python 
X_diff = X[:,:-1,:].copy()
X_diff[:,:,0] = np.diff(X[:,:,0])
X_diff[:,:,1] = np.diff(X[:,:,1])
</code></pre>
<p>At this point it is worthed to look at the relative movement amplitude distribution:</p>
<p><a href="/asset/images/deep_mouse/relative_movement_hist.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/relative_movement_hist.png" alt="Relative movement distribution" width="600" style="max-width:100%;"></a></p>
<p>As we can see, the mouse cursor movement in 10ms ranges between 0 and 25 pixels. More importantly, we notice that there is a significant amount of data-point in which the cursor does not move at all. This is due to the fact that during the recording, while reading, the track-pad is untouched. Being this king of data completely irrelevant for the hand recognition problem it is worthed to filter them out to reduce the noise on the dataset:</p>
<div class="highlight highlight-source-python"><pre>sigma_x <span class="pl-k">=</span> np.std(X_diff[:,:,<span class="pl-c1">0</span>], <span class="pl-v">axis</span><span class="pl-k">=</span><span class="pl-c1">1</span>)
sigma_y <span class="pl-k">=</span> np.std(X_diff[:,:,<span class="pl-c1">1</span>], <span class="pl-v">axis</span><span class="pl-k">=</span><span class="pl-c1">1</span>)

X_filt <span class="pl-k">=</span> X_diff[(sigma_x<span class="pl-k">&gt;</span><span class="pl-c1">0.1</span>)<span class="pl-k">*</span>(sigma_y<span class="pl-k">&gt;</span><span class="pl-c1">0.1</span>)]</pre></div>
<p>Finally we normalize the dataset along the <em>x</em> and <em>y</em> direction dividing the dataset by the corresponding standard deviation. This correction is not dynamical but it is calculated statically only once. This is needed because the train/dev/test sets might have different variances, but the correction needs to be always the same:</p>
<div class="highlight highlight-source-python"><pre>x_std <span class="pl-k">=</span> <span class="pl-c1">3.398</span> <span class="pl-c"><span class="pl-c">#</span> np.mean(np.std(X_filt[:,:,0], axis=1))</span>
y_std <span class="pl-k">=</span> <span class="pl-c1">2.926</span> <span class="pl-c"><span class="pl-c">#</span> np.mean(np.std(X_filt[:,:,1], axis=1))</span>

X_filt[:,:,<span class="pl-c1">0</span>] <span class="pl-k">=</span> X_filt[:,:,<span class="pl-c1">0</span>] <span class="pl-k">/</span> x_std
X_filt[:,:,<span class="pl-c1">1</span>] <span class="pl-k">=</span> X_filt[:,:,<span class="pl-c1">1</span>] <span class="pl-k">/</span> y_std</pre></div>
<p>Here is how it looks the movement amplitude distribution after the cleaning and the normalization:</p>
<p><a href="/asset/images/deep_mouse/relative_movement_hist_corrected.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/relative_movement_hist_corrected.png" alt="Relative movement distribution corrected" width="700" style="max-width:100%;"></a></p>
##### Evaluation of the model - 02[△](#iterations-index)
<p>Lets look at the confusion matrix again:</p>
<p><a href="/asset/images/deep_mouse/conf_matrix_02.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/conf_matrix_02.png" alt="Confusion matrix 02" width="300" style="max-width:100%;"></a></p>
The overall accuracy increased a lot from the *01-iteration*.
<h3>
<a id="user-content-algorithm-selection---03" class="anchor" href="#algorithm-selection---03" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Algorithm selection - 03<a href="#iterations-index">△</a>
</h3>
<p>We now want to test different optimizers for the training. Up to now we used Adam with a learning rate of <code>lr = 0.001</code> and standard beta parameters (<code>beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0</code>). Before changing the optimizer we want to explore different values for the learning rate. We tested learning rates in the list <code>[0.01, 0.001, 0.0001, 0.00001]</code>. For each lr we initialize the model five times (changing the random seed) and we averaged the results. The accuracy and the loss function are plotted below:</p>
<p><a href="/asset/images/deep_mouse/model_03_comp.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/model_03_comp.png" alt="Model accuracy loss 03" width="600" style="max-width:100%;"></a></p>
The smaller is the learning rate, the smoother is the evolution of the accuracy (and loss function). A learning rate of `0.0001` seems to be the best compromise between achieving good results and having a short training time.
<h5>
<a id="user-content-evaluation-of-the-model---03index" class="anchor" href="#evaluation-of-the-model---03index" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Evaluation of the model - 03<a href="#iterations-index">index</a>
</h5>
<p>To evaluate the model at this point, we used the best learning rate (<code>lr = 0.0001</code>) and we trained the network for 300 epochs instead of 200.</p>
<p><a href="/asset/images/deep_mouse/model_03.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/model_03.png" alt="Confusion matrix 02" width="700" style="max-width:100%;"></a></p>
Here we can recognize the typical pattern of overfitting: the accuracy on the dev set increases until we hit 100 epochs, then it starts to decrease again. The same pattern, although reversed, appears in the loss function. With this model and dataset, we reached an accuracy of about 75%. To try to improve this result we can try different neural network models.
<h3>
<a id="user-content-algorithm-selection---04" class="anchor" href="#algorithm-selection---04" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Algorithm selection - 04<a href="#iterations-index">△</a>
</h3>
<p>We now explore the performance differences for different batch sizes. We tested values in <code>[13, 32, 64]</code> and, for each value, we run the model 9 times using a cross-validation method. The result is showed in the figure below
:</p>
<p><a href="/asset/images/deep_mouse/model_04_comp.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/model_04_comp.png" alt="Model accuracy loss 04" width="700" style="max-width:100%;"></a></p>
The differences are minimal. We chose a batch size of 32 since it gives better results faster. 
<h3>
<a id="user-content-algorithm-selection---05" class="anchor" href="#algorithm-selection---05" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Algorithm selection - 05<a href="#iterations-index">△</a>
</h3>
<p>We now test a completely different strategy: using a support vector machine to classify our data. We used a bayesian optimization with cross-validation approach to find the optimal <code>gamma</code> and <code>C</code> parameters, but the best accuracy obtained is 62%, which is quite low if compared to the LSTM performance. We, therefore, go back to the recurrent NN strategy.</p>
<h3>
<a id="user-content-lstm-size-and-learning-rate-with-bayesopt---06" class="anchor" href="#lstm-size-and-learning-rate-with-bayesopt---06" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>LSTM size and learning rate with Bayesopt - 06<a href="#iterations-index">△</a>
</h3>
<p>We use a Bayesian optimization approach to find the optimal number of neurons in the LSTM network and the optimal learning rate. We select the validation accuracy as the target variable to optimize. On the left, we see the performance estimation for a different number of LSTM neurons, on the x-axis, and for different learning rates, on the y-axis (obtained with a gaussian kernel). On the right panel, we plot the uncertainty of the gaussian model. Look at <a href="https://www.andreaamico.eu/machine_learning/2019/05/08/bayesian_opt.html" rel="nofollow">this blog post</a> to have a short introduction on how to code Bayesian optimization.</p>
<p><a href="/asset/images/deep_mouse/bayesian_opt.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/bayesian_opt.png" alt="bayesian optimization" width="700" style="max-width:100%;"></a></p>
The optimal parameters seem to sit between 150 and 250 LSTM neurons, and a learning rate of about 1.5e-4.
<h3>
<a id="user-content-introducing-dropout---07" class="anchor" href="#introducing-dropout---07" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introducing dropout - 07<a href="#iterations-index">△</a>
</h3>
<p>We test if we can reduce the accuracy gap between training and validation by introducting dropout in the LSTM layer:</p>
<div class="highlight highlight-source-python"><pre>LSTM(grid[<span class="pl-s"><span class="pl-pds">'</span>LSTM_size<span class="pl-pds">'</span></span>][<span class="pl-c1">1</span>], <span class="pl-v">input_shape</span><span class="pl-k">=</span>grid[<span class="pl-s"><span class="pl-pds">'</span>input_shape<span class="pl-pds">'</span></span>][<span class="pl-c1">1</span>],
            <span class="pl-v">kernel_initializer</span><span class="pl-k">=</span>glorot_uniform(<span class="pl-v">seed</span><span class="pl-k">=</span>grid[<span class="pl-s"><span class="pl-pds">'</span>seed_model<span class="pl-pds">'</span></span>][<span class="pl-c1">1</span>]),
            <span class="pl-v">dropout</span><span class="pl-k">=</span>grid[<span class="pl-s"><span class="pl-pds">'</span>dropout<span class="pl-pds">'</span></span>][<span class="pl-c1">1</span>])</pre></div>
<p>We use a simple grid search to find the best dropout vs LSTM size.</p>
<p><a href="/asset/images/deep_mouse/LSTM_vs_dropout.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/LSTM_vs_dropout.png" alt="LSTM dropout" width="350" style="max-width:100%;"></a></p>
We do not detect significant improvements introduced by dropout. We confirm that an LSTM of at least 200 units to reach a validation accuracy higher than 70%. For the moment we will remove the dropout regularization from the model. 
<h3>
<a id="user-content-checking-training-set-size---08" class="anchor" href="#checking-training-set-size---08" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Checking training set size - 08<a href="#iterations-index">△</a>
</h3>
<p>To check if the size of the training dataset is limiting the final accuracy we study the validation accuracy by artificially limiting the training set size.</p>
<p><a href="/asset/images/deep_mouse/training_size.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/training_size.png" alt="training size" width="400" style="max-width:100%;"></a></p>
Up to 250 batches of training data the model are completely unusable: the efficiency is as good as a random guess. From 250 up to 430 the validation accuracy increases almost linearly and does not seem to saturate. This suggests that we are strongly limited by the size of our training dataset. acquiring more data seems to be a very promising path to improve our accuracy.
<h3>
<a id="user-content-new-bigger-dataset---09" class="anchor" href="#new-bigger-dataset---09" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>New, bigger, dataset - 09<a href="#iterations-index">△</a>
</h3>
<p>The previous section told us we need more training points. We constructed a new dataset obtained in a similar way but using a mouse instead of the touchpad. The movements are recorded while playing simple browser games like string.io. The new training size available is now of about 1800 different batches instead of 430. The result is terrible, the validation accuracy is incredibly small even with a git training set, never performing better than 0.6. Maybe the differences in the movements of the left and the right hands, using the mouse, are less sharp than the ones recorded using the touchpad. For the moment we will go back using the old touchpad dataset.</p>
<h3>
<a id="user-content-gru-instead-of-lstm---10" class="anchor" href="#gru-instead-of-lstm---10" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>GRU instead of LSTM - 10<a href="#iterations-index">△</a>
</h3>
<p>A popular variation to the LSTM network is the GRU network (Gated Recurrent Unit), which is generally faster to train. The first impression is very positive: the accuracy is good and looks much more stable. In addition to that, we also add the possibility to introduce a variable number of convolutional layers between the raw data and the GRU unit:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> keras.models <span class="pl-k">import</span> Sequential
<span class="pl-k">from</span> keras.layers <span class="pl-k">import</span> Dense

<span class="pl-k">from</span> keras.layers.convolutional <span class="pl-k">import</span> Conv1D
<span class="pl-k">from</span> keras.layers.convolutional <span class="pl-k">import</span> MaxPooling1D
<span class="pl-k">from</span> keras.initializers <span class="pl-k">import</span> glorot_uniform


<span class="pl-k">def</span> <span class="pl-en">create_model</span>(<span class="pl-smi">grid</span>):
    <span class="pl-k">if</span> grid[<span class="pl-s"><span class="pl-pds">'</span>GPU<span class="pl-pds">'</span></span>][<span class="pl-c1">1</span>] <span class="pl-k">==</span> <span class="pl-c1">1</span>:
        <span class="pl-k">from</span> keras.layers <span class="pl-k">import</span> CuDNNGRU
        recurrent_unit <span class="pl-k">=</span> CuDNNGRU(grid[<span class="pl-s"><span class="pl-pds">'</span>GRU_size<span class="pl-pds">'</span></span>][<span class="pl-c1">1</span>],
                   <span class="pl-v">kernel_initializer</span><span class="pl-k">=</span>glorot_uniform(<span class="pl-v">seed</span><span class="pl-k">=</span>grid[<span class="pl-s"><span class="pl-pds">'</span>seed_model<span class="pl-pds">'</span></span>][<span class="pl-c1">1</span>]))
    <span class="pl-k">else</span>:
        <span class="pl-k">from</span> keras.layers <span class="pl-k">import</span> <span class="pl-c1">GRU</span>
        recurrent_unit <span class="pl-k">=</span> GRU(grid[<span class="pl-s"><span class="pl-pds">'</span>GRU_size<span class="pl-pds">'</span></span>][<span class="pl-c1">1</span>],
                   <span class="pl-v">kernel_initializer</span><span class="pl-k">=</span>glorot_uniform(<span class="pl-v">seed</span><span class="pl-k">=</span>grid[<span class="pl-s"><span class="pl-pds">'</span>seed_model<span class="pl-pds">'</span></span>][<span class="pl-c1">1</span>]),
                   <span class="pl-v">dropout</span><span class="pl-k">=</span>grid[<span class="pl-s"><span class="pl-pds">'</span>dropout<span class="pl-pds">'</span></span>][<span class="pl-c1">1</span>])

    model <span class="pl-k">=</span> Sequential()
    <span class="pl-k">for</span> _ <span class="pl-k">in</span> <span class="pl-c1">range</span>(grid[<span class="pl-s"><span class="pl-pds">'</span>conv_layers<span class="pl-pds">'</span></span>][<span class="pl-c1">1</span>]):
        model.add(Conv1D(<span class="pl-v">filters</span><span class="pl-k">=</span><span class="pl-c1">32</span>, <span class="pl-v">kernel_size</span><span class="pl-k">=</span><span class="pl-c1">3</span>, <span class="pl-v">padding</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>same<span class="pl-pds">'</span></span>, <span class="pl-v">activation</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>relu<span class="pl-pds">'</span></span>))
        model.add(MaxPooling1D(<span class="pl-v">pool_size</span><span class="pl-k">=</span><span class="pl-c1">2</span>))
    model.add(recurrent_unit)
    model.add(Dense(<span class="pl-c1">1</span>, <span class="pl-v">activation</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>sigmoid<span class="pl-pds">'</span></span>, <span class="pl-v">kernel_initializer</span><span class="pl-k">=</span>glorot_uniform(<span class="pl-v">seed</span><span class="pl-k">=</span>grid[<span class="pl-s"><span class="pl-pds">'</span>seed_model<span class="pl-pds">'</span></span>][<span class="pl-c1">1</span>])))
    <span class="pl-k">return</span> model</pre></div>
<p>We start scanning the GRU size parameter, together with the number of convolutional layers:</p>
<p><a href="/asset/images/deep_mouse/conv_layer_number_contourf.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/conv_layer_number_contourf.png" alt="GRU vs conv layer" width="350" style="max-width:100%;"></a></p>
GRU unit size and the number of convolutional layers seem not to affect too much the overall validation accuracy, which is always found between 70 and 75%. To gain more insights on the effect of different network structure we plot the evolution of accuracy and validation accuracy during the training:
<p><a href="/asset/images/deep_mouse/gru_size_conv_layer.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/gru_size_conv_layer.png" alt="GRU size conv layer" width="750" style="max-width:100%;"></a></p>
We can see how the training accuracy behaves much better than before, greatly exceeding the 90% limit set by the previous model. At the same time, we notice that the validation accuracy saturates around the epoch 100, at about 70-75%. This seems to be a clear indication of overfitting. Our idea is therefore to introduce some kind of regularization (e.g. dropout).
<p>It is also interesting to isolate the effect of the presence of convolutional layers. In the following plot, we present the validation accuracy during the training for 0, 1, 2 and 3 convolutional layers between the input and the GRU unit.</p>
<p><a href="/asset/images/deep_mouse/conv_layer_number.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/conv_layer_number.png" alt="number of convolutional layers" width="750" style="max-width:100%;"></a></p>
The presence of convolutional layers seems to increase the learning speed, moreover leads to cleaner results: the thickness of the shadow showing the variance of the signal decreases. For the future, we set the number of convolutional layers to be 3. 
<h3>
<a id="user-content-gru-and-overfitting---11" class="anchor" href="#gru-and-overfitting---11" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>GRU and overfitting - 11<a href="#iterations-index">△</a>
</h3>
<p>Now the training accuracy is great, but the validation accuracy does not increase more than 70-75%. After around 75 epochs of training, the validation accuracy stops to increase. This is a symptom of overfitting. To try to fix this we changed the GRU unit size (to reduce the complexity of the model) and introduce two dropout steps: recurrent dropout within the GRU layer, and classic dropout in between the convolutional layers. As we can see in the plot below none of these techniques worked out, but actually, reduce the performance of the model.</p>
<p><a href="/asset/images/deep_mouse/model_11_overfit.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/model_11_overfit.png" alt="model 11 overfit" width="750" style="max-width:100%;"></a></p>
<h3>
<a id="user-content-gru-with-mouse-movement---12" class="anchor" href="#gru-with-mouse-movement---12" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>GRU with mouse movement - 12<a href="#iterations-index">△</a>
</h3>
<p>Since the bottleneck might still be the training size, we decide to try again our bigger dataset (the one obtained using mouse movements instead of touchpad ones). We study again the accuracy as a function of the training size.</p>
<p><a href="/asset/images/deep_mouse/training_size_mouse.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/training_size_mouse.png" alt="training size mouse" width="350" style="max-width:100%;"></a></p>
Similarly to the touchpad dataset the accuracy hits its maximum at around 500 batches of training. This time it is clear that having more training datapoints does not improve the performance of the model.
<h3>
<a id="user-content-training-size-and-regularizers---13" class="anchor" href="#training-size-and-regularizers---13" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Training size and regularizers - 13<a href="#iterations-index">△</a>
</h3>
<p>In the previous section, we realized that we are not limited by the size of the training data. Using the same training dataset we can try to train our model with bigger batches, which effectively reduces the number of batches available. At the same time it will increase the minimum time required for the model to perform a guess: twice the batch size, twice is the recording time needed. Nevertheless, the accuracy might increase.</p>
<p><a href="/asset/images/deep_mouse/batch_size_13.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/batch_size_13.png" alt="batch and training size" width="750" style="max-width:100%;"></a></p>
On the most left plot, we see how bigger batch size generally corresponds to an improvement of the model performance. The accuracy saturates around a batch size of 500. On the central plot and on the most right one, we test if the amount of training data is enought even for larger batches. The answer is positive since we observe saturation in both cases. With a batch size of 800, we reach a validation accuracy higher than 80%.
<p>Using a batch size of 800 we try to fill the gap between validation accuracy and training accuracy playing with the GRU unit size, the number of convolutional layers and the l1 regularizer within the GRU unit.</p>
<p><a href="/asset/images/deep_mouse/gru_conv_reg_13.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/deep_mouse/gru_conv_reg_13.png" alt="gru_size conv regularizer" width="750" style="max-width:100%;"></a></p>
None of those methods provide improvements to accuracy. The GRU unit size seems not to affect the accuracy substantially. The number of convolutional layers is relevant, but we were already using 3 of them, which corresponds to the best performance. The l1 regularization technique seems detrimental, always reducing the validation accuracy.
</article></body></html>