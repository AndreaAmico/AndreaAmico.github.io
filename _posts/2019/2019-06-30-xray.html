<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><style>@font-face {
  font-family: octicons-anchor;
  src: url(https://cdnjs.cloudflare.com/ajax/libs/octicons/4.4.0/font/octicons.woff) format('woff');
}

* {
    box-sizing: border-box;
}

body {
    width: 980px;
    margin-right: auto;
    margin-left: auto;
    color:#333;
    background:#fff;
}

body .markdown-body {
    padding: 45px;
    border: 1px solid #ddd;
    border-radius: 3px;
    word-wrap: break-word;
}

pre {
    font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body {
  -webkit-text-size-adjust: 100%;
  text-size-adjust: 100%;
  color: #333;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background-color: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body strong {
  font-weight: bold;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body input {
  font: 13px / 1.4 Helvetica, arial, nimbussansl, liberationsans, freesans, clean, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
}

.markdown-body a {
  color: #4078c0;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before {
  display: table;
  content: "";
}

.markdown-body hr:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
  font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body .select::-ms-expand {
  opacity: 0;
}

.markdown-body .octicon {
  font: normal normal normal 16px/1 octicons-anchor;
  display: inline-block;
  text-decoration: none;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.markdown-body .octicon-link:before {
  content: '\f05c';
}

.markdown-body:before {
  display: table;
  content: "";
}

.markdown-body:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body a:not([href]) {
  color: inherit;
  text-decoration: none;
}

.markdown-body .anchor {
  display: inline-block;
  padding-right: 2px;
  margin-left: -18px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  color: #000;
  vertical-align: middle;
  visibility: hidden;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  visibility: visible;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h1 .anchor {
  line-height: 1;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 .anchor {
  line-height: 1;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h3 .anchor {
  line-height: 1.2;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h4 .anchor {
  line-height: 1.2;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h5 .anchor {
  line-height: 1.1;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body h6 .anchor {
  line-height: 1.1;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  box-sizing: content-box;
  background-color: #fff;
}

.markdown-body code {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}

.markdown-body .pl-c {
  color: #969896;
}

.markdown-body .pl-c1,
.markdown-body .pl-s .pl-v {
  color: #0086b3;
}

.markdown-body .pl-e,
.markdown-body .pl-en {
  color: #795da3;
}

.markdown-body .pl-s .pl-s1,
.markdown-body .pl-smi {
  color: #333;
}

.markdown-body .pl-ent {
  color: #63a35c;
}

.markdown-body .pl-k {
  color: #a71d5d;
}

.markdown-body .pl-pds,
.markdown-body .pl-s,
.markdown-body .pl-s .pl-pse .pl-s1,
.markdown-body .pl-sr,
.markdown-body .pl-sr .pl-cce,
.markdown-body .pl-sr .pl-sra,
.markdown-body .pl-sr .pl-sre {
  color: #183691;
}

.markdown-body .pl-v {
  color: #ed6a43;
}

.markdown-body .pl-id {
  color: #b52a1d;
}

.markdown-body .pl-ii {
  background-color: #b52a1d;
  color: #f8f8f8;
}

.markdown-body .pl-sr .pl-cce {
  color: #63a35c;
  font-weight: bold;
}

.markdown-body .pl-ml {
  color: #693a17;
}

.markdown-body .pl-mh,
.markdown-body .pl-mh .pl-en,
.markdown-body .pl-ms {
  color: #1d3e81;
  font-weight: bold;
}

.markdown-body .pl-mq {
  color: #008080;
}

.markdown-body .pl-mi {
  color: #333;
  font-style: italic;
}

.markdown-body .pl-mb {
  color: #333;
  font-weight: bold;
}

.markdown-body .pl-md {
  background-color: #ffecec;
  color: #bd2c00;
}

.markdown-body .pl-mi1 {
  background-color: #eaffea;
  color: #55a532;
}

.markdown-body .pl-mdr {
  color: #795da3;
  font-weight: bold;
}

.markdown-body .pl-mo {
  color: #1d3e81;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}

.markdown-body .plan-price-unit {
  color: #767676;
  font-weight: normal;
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 0.35em 0.25em -1.6em;
  vertical-align: middle;
}

.markdown-body .plan-choice {
  padding: 15px;
  padding-left: 40px;
  display: block;
  border: 1px solid #e0e0e0;
  position: relative;
  font-weight: normal;
  background-color: #fafafa;
}

.markdown-body .plan-choice.open {
  background-color: #fff;
}

.markdown-body .plan-choice.open .plan-choice-seat-breakdown {
  display: block;
}

.markdown-body .plan-choice-free {
  border-radius: 3px 3px 0 0;
}

.markdown-body .plan-choice-paid {
  border-radius: 0 0 3px 3px;
  border-top: 0;
  margin-bottom: 20px;
}

.markdown-body .plan-choice-radio {
  position: absolute;
  left: 15px;
  top: 18px;
}

.markdown-body .plan-choice-exp {
  color: #999;
  font-size: 12px;
  margin-top: 5px;
}

.markdown-body .plan-choice-seat-breakdown {
  margin-top: 10px;
  display: none;
}

.markdown-body :checked+.radio-label {
  z-index: 1;
  position: relative;
  border-color: #4078c0;
}

@media print {
  body .markdown-body {
    padding: 0;
    border: none;
  }
}
</style><title>2019-06-30-xray</title></head><body><article class="markdown-body"><hr>
<h2>
<a id="user-content-layout-posttitle--dectection-of-pneumothorax-using-x-ray-imagesdate---2019-06-30-220000-0100categories-machine_learning" class="anchor" href="#layout-posttitle--dectection-of-pneumothorax-using-x-ray-imagesdate---2019-06-30-220000-0100categories-machine_learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>layout: post
title:  "Dectection of pneumothorax using x-ray images"
date:   2019-06-30 22:00:00 +0100
categories: machine_learning</h2>
<p><a href="/asset/images/xray/intro.png" target="_blank" rel="noopener noreferrer"><img src="/asset/images/xray/intro.png" alt="denoise example" width="300" style="max-width:100%;"></a></p>
<h3>
<a id="user-content-problem-definition" class="anchor" href="#problem-definition" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Problem definition</h3>
<p>In this post we will approach the problem of image segmentation using the pneumothorax dataset from <a href="https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation" rel="nofollow">kaggle</a>. The problem is straightforward: given a x-ray image of the patient's cest, you need detect the presence of pneumothorax issue and spatially locate the region of interest within the image. You are provided with around 100000 x-ray images for training. Each of them come with a spatial mask locating the pneumothorax problem in the image (if the lung is healthy the mask is just empty). An example of x-ray image is shown above: the right lung has complications in the top external region.</p>
<h3>
<a id="user-content-neural-network-model" class="anchor" href="#neural-network-model" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Neural network model</h3>
<p>We choose to approach this problem using a convolutional neural network (the good old friend of image analysis). Instead of training a network from scratch we decided to exploit VGG-16 network model, pretrained on the <a href="http://www.image-net.org/" rel="nofollow">imagenet</a> dataset. The idea is to exploit the first layers of this network, which are already trained to "understand" the very low-level features of an image, such as edges and extremely simple patterns. Since the imagenet dataset contains more than 14 millions of images, we want to exploit the ability of the trained VGG-16 network to extract the most important low lever features from an image, and transfer this "knowledge" to our problem.</p>
<p>Since we are interested in finding the position of the possible lung problem, it makes sense not to exit from the convolutional network structure using dense layers. Instead, one can use a U-type of network, similar to the one used in autoencoders: in the first section of the network we extrac the most importan features by applying convolutional layers and max pooling to reduce the size of the image representation and increase the number of channels. In the second half, instead, we use a combination of up-sampling and convolutional layers to reduce the number of channels to just one and increase the image rapresentation size back to the original one.</p>
<h3>
<a id="user-content-importing-the-images" class="anchor" href="#importing-the-images" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Importing the images</h3>
<p>First of all we need to import the training dataset and split it in train/dev/test sets. Here we create two lists containing the file paths of the training images together with the respective mask. Moreover, we shuffle these lists and we split them in train/dev/test with a proportion of 250/40/40.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> glob
<span class="pl-k">import</span> random
random.seed(<span class="pl-c1">2</span>)

xray_path <span class="pl-k">=</span> <span class="pl-s">f</span><span class="pl-pds">'</span><span class="pl-c1">{</span>root_path<span class="pl-c1">}</span><span class="pl-s">data/train/*</span><span class="pl-pds">'</span>
mask_path <span class="pl-k">=</span> <span class="pl-s">f</span><span class="pl-pds">'</span><span class="pl-c1">{</span>root_path<span class="pl-c1">}</span><span class="pl-s">data/masks/*</span><span class="pl-pds">'</span>
xray_files <span class="pl-k">=</span> <span class="pl-c1">sorted</span>(glob.glob(xray_path))
mask_files <span class="pl-k">=</span> <span class="pl-c1">sorted</span>(glob.glob(mask_path))

c <span class="pl-k">=</span> <span class="pl-c1">list</span>(<span class="pl-c1">zip</span>(xray_files, mask_files))
random.shuffle(c)
xray_files, mask_files <span class="pl-k">=</span> <span class="pl-c1">zip</span>(<span class="pl-k">*</span>c)

batch_size <span class="pl-k">=</span> <span class="pl-c1">32</span>

train_len <span class="pl-k">=</span> <span class="pl-c1">250</span> <span class="pl-k">*</span> batch_size
dev_len <span class="pl-k">=</span> <span class="pl-c1">40</span> <span class="pl-k">*</span> batch_size
test_len <span class="pl-k">=</span> <span class="pl-c1">40</span> <span class="pl-k">*</span> batch_size

X_train_files <span class="pl-k">=</span> xray_files[:train_len]
y_train_files <span class="pl-k">=</span> mask_files[:train_len]

X_dev_files <span class="pl-k">=</span> xray_files[train_len:train_len<span class="pl-k">+</span>dev_len]
y_dev_files <span class="pl-k">=</span> mask_files[train_len:train_len<span class="pl-k">+</span>dev_len]

X_test_files <span class="pl-k">=</span> xray_files[train_len<span class="pl-k">+</span>dev_len:train_len<span class="pl-k">+</span>dev_len<span class="pl-k">+</span>test_len]
y_test_files <span class="pl-k">=</span> mask_files[train_len<span class="pl-k">+</span>dev_len:train_len<span class="pl-k">+</span>dev_len<span class="pl-k">+</span>test_len]</pre></div>
<p>Then, we want to make sure that the train/dev/test contains image with zero mask (without any detectable lung problem) in the same proportion.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> keras.preprocessing.image <span class="pl-k">import</span> load_img
<span class="pl-k">from</span> keras.preprocessing.image <span class="pl-k">import</span> img_to_array

with_mask <span class="pl-k">=</span> []
<span class="pl-k">for</span> i, f <span class="pl-k">in</span> <span class="pl-c1">enumerate</span>(y_train_files):
    m <span class="pl-k">=</span> np.sum(img_to_array(load_img(f, <span class="pl-v">color_mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>grayscale<span class="pl-pds">'</span></span>)))
    with_mask.append(m<span class="pl-k">&gt;</span><span class="pl-c1">1</span>)
train_positive <span class="pl-k">=</span> np.mean(with_mask)

with_mask <span class="pl-k">=</span> []
<span class="pl-k">for</span> i, f <span class="pl-k">in</span> <span class="pl-c1">enumerate</span>(y_dev_files):
    m <span class="pl-k">=</span> np.sum(img_to_array(load_img(f, <span class="pl-v">color_mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>grayscale<span class="pl-pds">'</span></span>)))
    with_mask.append(m<span class="pl-k">&gt;</span><span class="pl-c1">1</span>)
dev_positive <span class="pl-k">=</span> np.mean(with_mask)

with_mask <span class="pl-k">=</span> []
<span class="pl-k">for</span> i, f <span class="pl-k">in</span> <span class="pl-c1">enumerate</span>(y_test_files):
    m <span class="pl-k">=</span> np.sum(img_to_array(load_img(f, <span class="pl-v">color_mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>grayscale<span class="pl-pds">'</span></span>)))
    with_mask.append(m<span class="pl-k">&gt;</span><span class="pl-c1">1</span>)
test_positive <span class="pl-k">=</span> np.mean(with_mask)

<span class="pl-c1">print</span>(<span class="pl-s"><span class="pl-pds">'</span>Images with mask:<span class="pl-pds">'</span></span>)
<span class="pl-c1">print</span>(<span class="pl-s">f</span><span class="pl-pds">'</span><span class="pl-s">Train:</span><span class="pl-c1">{</span>train_positive<span class="pl-c1">}</span><span class="pl-s">, dev:</span><span class="pl-c1">{</span>dev_positive<span class="pl-c1">}</span><span class="pl-s">, test:</span><span class="pl-c1">{</span>test_positive<span class="pl-c1">}</span><span class="pl-pds">'</span>)</pre></div>
<pre lang="text"><code>Images with mask:
Train:0.22075, dev:0.22734375, test:0.2296875
</code></pre>
<p>As we can see the split is good: train, dev and test sets contains about the same percentage of images with lung issue (arouns 22-23%).</p>
<h3>
<a id="user-content-using-google-colab" class="anchor" href="#using-google-colab" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using Google colab</h3>
<p>Since at the moment we have no acces to a local GPU, which is essential to train a conv net, we exploit Google colab free jupyter notebook. We found out the major limitation of this approach being the low speed in loading the images from the hard drive to the memory, resulting in a very severe performance issue. When using Colab one can store the training files on a google drive folder and load them directly from there. The problem rise as soon as one try to load more then a couple of hundreds of images: at first the process is quite fast and requires for 5 to 10 ms for each image, which is ok, but, after a while, the speed drastically drop (apparently for no reason) and the loading time becomes of the order of half of a second, an extremely huge bottleneck in the pipeline which practically results in the freezing of the training computation. Since the dataset is quite small in size (of the order of Gb), we decided to load it all in memory. We used the <a href="https://en.wikipedia.org/wiki/Hierarchical_Data_Format" rel="nofollow">HDF5</a> data format to store all the dataset, and load just this one big file on our google drive folder.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> h5py

data_filename <span class="pl-k">=</span> <span class="pl-s">f</span><span class="pl-pds">'</span><span class="pl-c1">{</span>root_path<span class="pl-c1">}</span><span class="pl-s">data/data.h5</span><span class="pl-pds">'</span>
<span class="pl-k">with</span> h5py.File(data_filename, <span class="pl-s"><span class="pl-pds">"</span>w<span class="pl-pds">"</span></span>) <span class="pl-k">as</span> out:
    
    data_type <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">'</span>uint8<span class="pl-pds">'</span></span>
    
    out.create_dataset(<span class="pl-s"><span class="pl-pds">"</span>X_train<span class="pl-pds">"</span></span>,(<span class="pl-c1">len</span>(X_train_files),<span class="pl-c1">256</span>,<span class="pl-c1">256</span>,<span class="pl-c1">1</span>),<span class="pl-v">dtype</span><span class="pl-k">=</span>data_type)
    out.create_dataset(<span class="pl-s"><span class="pl-pds">"</span>Y_train<span class="pl-pds">"</span></span>,(<span class="pl-c1">len</span>(X_train_files),<span class="pl-c1">256</span>,<span class="pl-c1">256</span>,<span class="pl-c1">1</span>),<span class="pl-v">dtype</span><span class="pl-k">=</span>data_type)      
    out.create_dataset(<span class="pl-s"><span class="pl-pds">"</span>X_dev<span class="pl-pds">"</span></span>,(<span class="pl-c1">len</span>(X_dev_files),<span class="pl-c1">256</span>,<span class="pl-c1">256</span>,<span class="pl-c1">1</span>),<span class="pl-v">dtype</span><span class="pl-k">=</span>data_type)
    out.create_dataset(<span class="pl-s"><span class="pl-pds">"</span>Y_dev<span class="pl-pds">"</span></span>,(<span class="pl-c1">len</span>(X_dev_files),<span class="pl-c1">256</span>,<span class="pl-c1">256</span>,<span class="pl-c1">1</span>),<span class="pl-v">dtype</span><span class="pl-k">=</span>data_type)      
    out.create_dataset(<span class="pl-s"><span class="pl-pds">"</span>X_test<span class="pl-pds">"</span></span>,(<span class="pl-c1">len</span>(X_test_files),<span class="pl-c1">256</span>,<span class="pl-c1">256</span>,<span class="pl-c1">1</span>),<span class="pl-v">dtype</span><span class="pl-k">=</span>data_type)
    out.create_dataset(<span class="pl-s"><span class="pl-pds">"</span>Y_test<span class="pl-pds">"</span></span>,(<span class="pl-c1">len</span>(X_test_files),<span class="pl-c1">256</span>,<span class="pl-c1">256</span>,<span class="pl-c1">1</span>),<span class="pl-v">dtype</span><span class="pl-k">=</span>data_type)
    
    <span class="pl-k">for</span> index <span class="pl-k">in</span> <span class="pl-c1">range</span>(<span class="pl-c1">len</span>(y_train_files)):
        out[<span class="pl-s"><span class="pl-pds">'</span>X_train<span class="pl-pds">'</span></span>][index, :, :, :] <span class="pl-k">=</span> img_to_array(load_img(
            X_train_files[index], <span class="pl-v">color_mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>grayscale<span class="pl-pds">'</span></span>)).astype(data_type)
        out[<span class="pl-s"><span class="pl-pds">'</span>Y_train<span class="pl-pds">'</span></span>][index, :, :, :] <span class="pl-k">=</span> img_to_array(load_img(
            y_train_files[index], <span class="pl-v">color_mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>grayscale<span class="pl-pds">'</span></span>)).astype(data_type)
        
    <span class="pl-k">for</span> index <span class="pl-k">in</span> <span class="pl-c1">range</span>(<span class="pl-c1">len</span>(y_dev_files)):
        out[<span class="pl-s"><span class="pl-pds">'</span>X_dev<span class="pl-pds">'</span></span>][index, :, :, :] <span class="pl-k">=</span> img_to_array(load_img(
            X_dev_files[index], <span class="pl-v">color_mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>grayscale<span class="pl-pds">'</span></span>)).astype(data_type)
        out[<span class="pl-s"><span class="pl-pds">'</span>Y_dev<span class="pl-pds">'</span></span>][index, :, :, :] <span class="pl-k">=</span> img_to_array(load_img(
            y_dev_files[index], <span class="pl-v">color_mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>grayscale<span class="pl-pds">'</span></span>)).astype(data_type)
        
    <span class="pl-k">for</span> index <span class="pl-k">in</span> <span class="pl-c1">range</span>(<span class="pl-c1">len</span>(y_test_files)):
        out[<span class="pl-s"><span class="pl-pds">'</span>X_test<span class="pl-pds">'</span></span>][index, :, :, :] <span class="pl-k">=</span> img_to_array(load_img(
            X_test_files[index], <span class="pl-v">color_mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>grayscale<span class="pl-pds">'</span></span>)).astype(data_type)
        out[<span class="pl-s"><span class="pl-pds">'</span>Y_test<span class="pl-pds">'</span></span>][index, :, :, :] <span class="pl-k">=</span> img_to_array(load_img(
            y_test_files[index], <span class="pl-v">color_mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>grayscale<span class="pl-pds">'</span></span>)).astype(data_type)

</pre></div>
<h3>
<a id="user-content-data-augmentation" class="anchor" href="#data-augmentation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data augmentation</h3>
<p>In order to avoid overfittig we decide to use a data augmentation technique widely used during the training of neural networks for cvomputer vision tasks. The idea is never to train the network with exacly the same image twice: every time the image is presented to the network to perform one gradient descent step, it is randomly modified by a given transformation chosen at random within a given set. To do this we explot the `ImageDataGenerator builtin function in the Keras library:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> keras.preprocessing.image <span class="pl-k">import</span> ImageDataGenerator

image_randomiser <span class="pl-k">=</span> ImageDataGenerator(
    <span class="pl-v">rotation_range</span><span class="pl-k">=</span><span class="pl-c1">10</span>,
    <span class="pl-v">width_shift_range</span><span class="pl-k">=</span><span class="pl-c1">0.02</span>,
    <span class="pl-v">height_shift_range</span><span class="pl-k">=</span><span class="pl-c1">0.02</span>,
    <span class="pl-v">zoom_range</span><span class="pl-k">=</span><span class="pl-c1">0.02</span>,
    <span class="pl-v">fill_mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>nearest<span class="pl-pds">'</span></span>,
    <span class="pl-v">horizontal_flip</span><span class="pl-k">=</span><span class="pl-c1">True</span>)</pre></div>
</article></body></html>